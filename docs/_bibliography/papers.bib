---
---

@string{aps = {American Physical Society,}}


@inproceedings{menezes_bias_2021,
	address = {Gramado, Rio Grande do Sul, Brazil},
	title = {Bias and {Fairness} in {Face} {Detection}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-66542-354-0},
	url = {https://ieeexplore.ieee.org/document/9643102/},
	doi = {10.1109/SIBGRAPI54419.2021.00041},
	abstract = {Processing of face images is used in many areas, for example: commercial applications such as video-games; facial biometrics; facial expression recognition, etc. Face detection is a crucial step for any system that processes face images. Therefore, if there is bias or unfairness in this ﬁrst step, all the processing steps that follow may be compromised. Errors in automatic face detection may be harmful to people as, for instance, in situations where a decision may limit or restrict their freedom to come and go. Therefore, it is crucial to investigate the existence of these errors caused due to bias or unfairness. In this paper, an analysis of ﬁve well-known top accuracy face detectors is performed to investigate the presence of bias and unfairness in their results. Some of the metrics used to identify the existence of bias and unfairness involved the veriﬁcation of demographic parity, veriﬁcation of existence of false positives and/or false negatives, rate of positive prediction, and veriﬁcation of equalized odds. Data from about 365 different individuals were randomly selected from the Facebook Casual Conversations Dataset, resulting in approximately 5,500 videos, providing 550,000 frames used for face detection in the performed experiments. The obtained results show that all ﬁve face detectors presented a high risk of not detecting faces from the female gender and from people between 46 and 85 years old. Furthermore, the skin tone groups related with dark skin are the groups pointed out with highest risk of faces not being detected for four of the ﬁve evaluated face detectors. This paper points out the necessity of the research community to engage in breaking the perpetuation of injustice that may be present in datasets or machine learning models.},
	language = {en},
	urldate = {2024-05-16},
	booktitle = {2021 34th {SIBGRAPI} {Conference} on {Graphics}, {Patterns} and {Images} ({SIBGRAPI})},
	publisher = {IEEE},
	author = {Menezes, Hanna F. and Ferreira, Arthur S. C. and Pereira, Eanes T. and Gomes, Herman M.},
	month = oct,
	year = {2021},
	pages = {247--254},
	file = {Menezes et al. - 2021 - Bias and Fairness in Face Detection.pdf:C\:\\Users\\BigPatate\\Zotero\\storage\\T37L2G39\\Menezes et al. - 2021 - Bias and Fairness in Face Detection.pdf:application/pdf},
}

@inproceedings{buolamwini_gender_2018,
	address = {New York, NY, USA},
	title = {Gender {Shades}: {Intersectional} {Accuracy} {Disparities} in {Commercial} {Gender} {Classiﬁcation}},
	abstract = {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist approved Fitzpatrick Skin Type classiﬁcation system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We ﬁnd that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6\% for IJB-A and 86.2\% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classiﬁcation systems using our dataset and show that darker-skinned females are the most misclassiﬁed group (with error rates of up to 34.7\%). The maximum error rate for lighter-skinned males is 0.8\%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classiﬁcation systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.},
	language = {en},
	booktitle = {Conference on {Fairness}, {Accountability}, and {Transparency},},
	author = {Buolamwini, Joy and Gebru, Timnit},
	month = feb,
	year = {2018},
	keywords = {Skimmed, NEED TO READ},
	file = {Buolamwini and Gebru - Gender Shades Intersectional Accuracy Disparities.pdf:C\:\\Users\\BigPatate\\Zotero\\storage\\BURC9WVT\\Buolamwini and Gebru - Gender Shades Intersectional Accuracy Disparities.pdf:application/pdf},
}

@misc{hazirbas_towards_2021,
	title = {Towards {Measuring} {Fairness} in {AI}: the {Casual} {Conversations} {Dataset}},
	shorttitle = {Towards {Measuring} {Fairness} in {AI}},
	url = {http://arxiv.org/abs/2104.02821},
	doi = {10.48550/arXiv.2104.02821},
	abstract = {This paper introduces a novel dataset to help researchers evaluate their computer vision and audio models for accuracy across a diverse set of age, genders, apparent skin tones and ambient lighting conditions. Our dataset is composed of 3,011 subjects and contains over 45,000 videos, with an average of 15 videos per person. The videos were recorded in multiple U.S. states with a diverse set of adults in various age, gender and apparent skin tone groups. A key feature is that each subject agreed to participate for their likenesses to be used. Additionally, our age and gender annotations are provided by the subjects themselves. A group of trained annotators labeled the subjects’ apparent skin tone using the Fitzpatrick skin type scale. Moreover, annotations for videos recorded in low ambient lighting are also provided. As an application to measure robustness of predictions across certain attributes, we provide a comprehensive study on the top ﬁve winners of the DeepFake Detection Challenge (DFDC). Experimental evaluation shows that the winning models are less performant on some speciﬁc groups of people, such as subjects with darker skin tones and thus may not generalize to all people. In addition, we also evaluate the state-of-the-art apparent age and gender classiﬁcation methods. Our experiments provides a thorough analysis on these models in terms of fair treatment of people from various backgrounds.},
	language = {en},
	urldate = {2025-02-19},
	publisher = {arXiv},
	author = {Hazirbas, Caner and Bitton, Joanna and Dolhansky, Brian and Pan, Jacqueline and Gordo, Albert and Ferrer, Cristian Canton},
	month = nov,
	year = {2021},
	note = {arXiv:2104.02821 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Hazirbas et al. - 2021 - Towards Measuring Fairness in AI the Casual Conve.pdf:C\:\\Users\\BigPatate\\Zotero\\storage\\XD4GVRLM\\Hazirbas et al. - 2021 - Towards Measuring Fairness in AI the Casual Conve.pdf:application/pdf},
}

@article{khalil_investigating_2020,
	title = {Investigating {Bias} in {Facial} {Analysis} {Systems}: {A} {Systematic} {Review}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {Investigating {Bias} in {Facial} {Analysis} {Systems}},
	url = {https://ieeexplore.ieee.org/document/9130131/?arnumber=9130131},
	doi = {10.1109/ACCESS.2020.3006051},
	abstract = {Recent studies have demonstrated that most commercial facial analysis systems are biased against certain categories of race, ethnicity, culture, age and gender. The bias can be traced in some cases to the algorithms used and in other cases to insufficient training of algorithms, while in still other cases bias can be traced to insufficient databases. To date, no comprehensive literature review exists which systematically investigates bias and discrimination in the currently available facial analysis software. To address the gap, this study conducts a systematic literature review (SLR) in which the context of facial analysis system bias is investigated in detail. The review, involving 24 studies, additionally aims to identify (a) facial analysis databases that were created to alleviate bias, (b) the full range of bias in facial analysis software and (c) algorithms and techniques implemented to mitigate bias in facial analysis.},
	urldate = {2025-02-24},
	journal = {IEEE Access},
	author = {Khalil, Ashraf and Ahmed, Soha Glal and Khattak, Asad Masood and Al-Qirim, Nabeel},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {NEED TO READ, Face, Face recognition, Algorithmic discrimination, bias, classification bias, Data mining, Databases, facial analysis, Software, Software algorithms, Systematics, unfairness},
	pages = {130751--130761},
	file = {Full Text PDF:C\:\\Users\\BigPatate\\Zotero\\storage\\Z3M482GD\\Khalil et al. - 2020 - Investigating Bias in Facial Analysis Systems A S.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\BigPatate\\Zotero\\storage\\SBLE6XQ2\\9130131.html:text/html},
}


@inproceedings{yang_enhancing_2022,
	address = {Oxford United Kingdom},
	title = {Enhancing {Fairness} in {Face} {Detection} in {Computer} {Vision} {Systems} by {Demographic} {Bias} {Mitigation}},
	isbn = {978-1-4503-9247-1},
	url = {https://dl.acm.org/doi/10.1145/3514094.3534153},
	doi = {10.1145/3514094.3534153},
	abstract = {Fairness has become an important agenda in computer vision and artificial intelligence. Recent studies have shown that many computer vision models and datasets exhibit demographic biases and proposed mitigation strategies. These works attempt to address accuracy disparity, spurious correlations, or unbalanced representations in datasets in tasks such as face recognition, verification and expression and attribute classification. These tasks, however, all require face detection as the first preprocessing step, and surprisingly, there has been little effort in identifying or mitigating biases in face detection. Biased face detectors themselves pose a threat against fair and ethical AI systems, and their biases may be further passed on to subsequent downstream tasks such as face recognition in a computer vision pipeline. This paper therefore investigates the problem of biases in face detection, focusing on accuracy disparity of detectors between demographic groups including gender, age group, and skin tone. We collect perceived demographic attributes on a popular face detection benchmark dataset, WIDER FACE, report skewed demographic distributions, and compare detection performance between groups. In order to mitigate the biases, we apply three mitigation methods that have been introduced in the recent literature and also propose two novel methods. Experimental results show that these methods are effective in reducing demographic biases. We also discuss how the effectiveness varies by demographic attributes, detection easiness, and multiple detectors, which will shed light on this new topic of addressing face detection bias.},
	language = {en},
	urldate = {2024-08-14},
	booktitle = {Proceedings of the 2022 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Yang, Yu and Gupta, Aayush and Feng, Jianwei and Singhal, Prateek and Yadav, Vivek and Wu, Yue and Natarajan, Pradeep and Hedau, Varsha and Joo, Jungseock},
	month = jul,
	year = {2022},
	pages = {813--822},
	file = {Yang et al. - 2022 - Enhancing Fairness in Face Detection in Computer V.pdf:C\:\\Users\\BigPatate\\Zotero\\storage\\FQZ8N6GW\\Yang et al. - 2022 - Enhancing Fairness in Face Detection in Computer V.pdf:application/pdf},
}